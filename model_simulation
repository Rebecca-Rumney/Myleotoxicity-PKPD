import numpy as np
import matplotlib.pyplot as plt
from scipy import integrate
from scipy import linalg
from collections.abc import Iterable
import pints

## PharmacoKinetics

def central_dydt(current_state, params):
    dC_dt = (params["k_dose"]*current_state["dose"]
            + sum(params["Q_perif"]*(current_state["peripheral"]/params["Vol_perif"]))
            - sum(params["Q_perif"]*(current_state["central"]/params["Vol_C"]))
            - params["Cl"]*(current_state["central"]/params["Vol_C"]))
    return dC_dt

def perif_dydt(current_state, params):
    dP_dt = (params["Q_perif"]*(current_state["central"]/params["Vol_C"])
            - params["Q_perif"]*(current_state["peripheral"]/params["Vol_perif"]))
    return dP_dt

def dose_iv(t):
    return 0
    
def PK_iv_dydt(y, t, param_values, comp=2):
    if comp==1:
        current_state= {'central': y, "peripheral":np.asarray([])}
        current_state["dose"] = dose_iv(t)
        params = {"k_dose" : 1, "Vol_C" : param_values[0], "Cl" : param_values[1], "Vol_perif" : np.asarray([]), "Q_perif" : np.asarray([])}
        
        dy_dt = central_dydt(current_state, params)
        
    else:
        current_state= {'central': y[0], "peripheral":np.asarray(y[1:])}
        current_state["dose"] = dose_iv(t)
        params= {"k_dose" : 1, "Vol_C" : param_values[0], "Cl" : param_values[1], "Vol_perif" : np.asarray(param_values[2]), "Q_perif" : np.asarray(param_values[3])}
        
        dC_dt = central_dydt(current_state, params)
        dP_dt = perif_dydt(current_state, params)
        dy_dt = np.insert(dP_dt, [0], [dC_dt])
    
    return dy_dt

def PK_iv_result(dose, num_comp, parameter, times):
    if len(parameter) != 2*num_comp:
        raise ValueError(
                'Number of Parameters does not match the number of Compartments')
    Vol_C = parameter[0]
    Cl = parameter[1]
    Vol_perif = parameter[2:num_comp+1]
    Q_perif = parameter[num_comp+1:]
    y_0 = [dose] +[0]*(num_comp-1)
    results_amt = integrate.odeint(PK_iv_dydt, y_0, times, args=((Vol_C, Cl, Vol_perif, Q_perif), num_comp))
    results_conc = results_amt/np.insert(Vol_perif, [0], Vol_C)
    return results_conc

if __name__ == '__main__':
    # Params:
    k_dose = 1
    k_perif = 0.5
    Vol_perif = 1
    Cl = 0.5
    Vol_C = 2

    dose = 5
    time_span = 10
    times = np.linspace(0,time_span,1000)
    plt.plot(times, PK_iv_result(dose, 3, [Vol_C, Cl, Vol_perif, Vol_perif*0.5, k_perif, k_perif+0.1], times)[:,:])
    plt.show

### Analytical Solution

This is not neccessary for inferring the PK parameters but it will make calculating PD easier.

def solve_2_comp(dose, params, t):
    if isinstance(t, Iterable)==False and t<0:
        return 0
    else:
        Vol_C = params[0]
        Cl = params[1]
        Vol_perif = params[2]
        Q = params[3]

        A = np.asarray([[-(Cl+Q)/Vol_C, Q/Vol_perif], [Q/Vol_C, -Q/Vol_perif]])
        x_0 = np.array([dose, 0])
        eigvalues, eigvectors = linalg.eig(A)

        if eigvalues[0]==eigvalues[1]:
            l = eigvalues[0]
            nu = eigvectors[0]
            rho = linalg.solve(A-l*np.identity(2), nu)
            c = linalg.solve([[nu[0], rho[0]],[nu[1], rho[1]]], x_0)
            C_t = c[0]*np.exp(l*t)*nu[0] + c[1]*(t*np.exp(l*t)*nu[0] +np.exp(l*t)*rho[0])
            
        elif eigvalues[0].imag == 0:
            l1, l2 = eigvalues
            nu1, nu2 = eigvectors
            c = linalg.solve([[nu1[0], nu2[0]],[nu1[1], nu2[1]]], x_0)
            C_t = c[0]*np.exp(l1*t)*nu1[0] + c[1]*np.exp(l2*t)*nu2[0]
            
        else:
            b = eigvalues[0].real
            a = eigvalues[0].imag
            nu = eigvectors[0]
            def u_v(time):
                ut_plus_ivt = np.exp(b*time)*(np.cos(a*time) + j*np.sin(b*time))*nu
                return (ut_plus_ivt.real, ut_plus_ivt.imag)
            
            u_0 = u_v(0)[0]
            v_0 = u_v(0)[1]
            c = linalg.solve([[u_0[0], v_0[0]],[u_0[1], v_0[1]]], x_0)
            C_t = c[0]*u_v(t)[0] + c[1]*u_v(t)[1]
        
        if isinstance(t, Iterable):
            times_before_dose = np.count_nonzero(t < 0)
            C_t[:times_before_dose] = 0
        return C_t.real/Vol_C

    
if __name__ == '__main__':
    
    Q_perif = 1
    Vol_perif = 1
    Cl = 1
    Vol_C = 2
    dose = 2
    
    times = np.linspace(-2, 48, 100)
    times_before_dose = np.count_nonzero(times < 0)
    int_times = np.concatenate((np.zeros(1), times[times_before_dose:]))
    
    plt.plot(times, solve_2_comp(dose, [Vol_C, Cl, Vol_perif, Q_perif], times), label='Analytical')
    plt.plot(int_times, PK_iv_result(dose, 2, [Vol_C, Cl, Vol_perif, Q_perif], int_times)[:,0], label = 'ODEint')
    plt.legend()
    plt.show

## Pharmacodynamics

def proliferation_dydt(current_state, PD_Params):
    dProl_dt = (PD_Params["k_prol"]*current_state["Prol"]
                *(1-E_Drug(current_state["Conc"], PD_Params["slope"]))
                *np.power((PD_Params["Circ_0"]/current_state["Circ"]), PD_Params["gamma"])
                - PD_Params["k_tr"]*current_state["Prol"])
    return dProl_dt
    
def transit_dydt(current_state, PD_Params):
    dT1_dt = PD_Params["k_tr"]*current_state["Prol"] - PD_Params["k_tr"]*current_state["T1"]
    dT2_dt = PD_Params["k_tr"]*current_state["T1"] - PD_Params["k_tr"]*current_state["T2"]
    dT3_dt = PD_Params["k_tr"]*current_state["T2"] - PD_Params["k_tr"]*current_state["T3"]
    return (dT1_dt, dT2_dt, dT3_dt)

def circulation_dydt(current_state, PD_Params):
    dCirc_dt = PD_Params["k_tr"]*current_state["T3"] - PD_Params["k_circ"]*current_state["Circ"]
    return dCirc_dt

def PD(y, t, PD_params, PK_params, num_comp, dose):
    variables = ["Prol", "T1", "T2", "T3", "Circ"]
    current_state = dict(zip(variables, y))
    current_state["Conc"] = drug_conc(t, PK_params, num_comp, dose)*1000
    
    parameter_names = ["k_tr", "k_prol", "k_circ", "Circ_0", "gamma", "slope"]
    PD_params = dict(zip(parameter_names, PD_params))
    
    dProl_dt = proliferation_dydt(current_state, PD_params)
    dCirc_dt = circulation_dydt(current_state, PD_params)
    dT1_dt, dT2_dt, dT3_dt = transit_dydt(current_state, PD_params)
    return [dProl_dt, dT1_dt, dT2_dt, dT3_dt, dCirc_dt]

def PD_result(dose, num_comp, parameter, times):
    if len(parameter) != 2*num_comp+4:
        raise ValueError(
                str(len(parameter)) + 'Parameters does not match the number of Compartments')
    PK_params = parameter[:2*num_comp]
    
    n=3
    Circ_0 = parameter[2*num_comp]
    MTT = parameter[2*num_comp+1]
    gamma = parameter[2*num_comp+2]
    slope = parameter[2*num_comp+3]
    
    k_tr = (n+1)/MTT
    k_prol = k_tr
    k_circ = k_tr
    
    y_0 = [Circ_0]*5
    PD_params = [k_tr, k_prol, k_circ, Circ_0, gamma, slope]
    times_before_dose = np.count_nonzero(times < 0)
    
    results_1 = integrate.odeint(PD, y_0, np.concatenate((times[:times_before_dose], np.asarray([0]))), args=(PD_params, PK_params, 2, dose))
    results_2 = integrate.odeint(PD, results_1[-1,:], np.concatenate((np.asarray([0]),times[times_before_dose:])), args=(PD_params, PK_params, 2, dose))
    results_conc = np.concatenate((results_1[:-1,4], results_2[1:,4]))
    return results_conc

def drug_conc(t, PK_params, num_comp, dose):
    if num_comp == 2:
        conc = solve_2_comp(dose, PK_params, t)
    return conc

def E_Drug(conc, slope):
    return slope*conc
    

if __name__ == '__main__':
    # Params:
    
    Circ_0 = 5.45
    MTT = 135
    gamma = 0.174
    slope = 0.126

    n = 3
    k_tr = (n+1)/MTT
    k_prol = k_tr
    k_circ = k_tr
    dose = 2
    
    PK_params = np.load('simulated_parameters_actual_dose'+str(dose)+'.npy')
    print("PK Parameters:",PK_params)
    PD_params_1 = [k_tr, k_prol, k_circ, Circ_0, gamma, slope]
    print("PD Parameters:",PD_params_1)
    PD_params_2 = [Circ_0, MTT, gamma, slope]
    print("PD Parameters:",PD_params_2)


    times = np.linspace(-0.1,1440,1441)
    y_0 = [Circ_0]*5
    results = integrate.odeint(PD, y_0, times, args=(PD_params_1, PK_params, 2, dose))
    plt.plot(times/24, results[:, 4])
    
    times = np.linspace(-48,1440,1489)
    parameters = np.concatenate((PK_params, np.asarray(PD_params_2)))
    results = PD_result(dose, 2, parameters, times)
    plt.plot(times/24, results[:])
    plt.show

class PintsPDFriberg(pints.ForwardModel):
    def __init__(self, PK_params, dose, num_comp=2, start_time=None):
        super(PintsPDFriberg, self).__init__()
        self._PK_params = PK_params
        self._dose = dose
        self._num_comp = num_comp
        if start_time==None:
            self._start_time = 0
        else:
            self._start_time = start_time
        
    def n_parameters(self):
        return 4
    
    def simulate(self, parameter, times):
        times = times + self._start_time
        all_params = np.concatenate((self._PK_params, np.asarray(parameter)))
        curve = PD_result(self._dose, self._num_comp, all_params, times)
        return curve

class PintsPDFribergFixParam(pints.ForwardModel):
    def __init__(self, PK_params, dose, fix_param, num_comp=2, start_time=None):
        super(PintsPDFribergFixParam, self).__init__()
        self._PK_params = PK_params
        self._dose = dose
        self._num_comp = num_comp
        self._fix_param = fix_param
        if start_time==None:
            self._start_time = 0
        else:
            self._start_time = start_time
        
    def n_parameters(self):
        return 4 - len(self._fix_param[0])
    
    def simulate(self, parameter, times):
        times = times + self._start_time
        PD_params = parameter
        param_pos = self._fix_param[0] # 
        indexes = np.argsort(self._fix_param[0])
        param_value = self._fix_param[1]
        
        for i in indexes:
            PD_params = np.insert(PD_params, param_pos[i], param_value[i])
        all_params = np.concatenate((self._PK_params, PD_params))
        curve = PD_result(self._dose, self._num_comp, all_params, times)
        return curve

## Error measures and Likelihoods

Now we will define the Error measures and likelihood functions that will be used in the inference of these two models. We will be using some that are already implemented in the PINTS package but I also wish to alter them or add my own. We will also be needing a way to return the pointwise log likelihoods.

### Constant Gaussian likelihood

If we assume that the distrbution of observations around the true value is given by a Gaussian distribution, i.e. 
$$
X^{obs} \sim f(t, \theta) + \sigma N(0,1),
$$

then the likelihood of our data is given by

$$ \log L(\theta, \sigma | X^{obs}) = \sum_{i=1}^{n_t}\sum_{j=1}^{n_t}\left[\log L(\theta,\sigma|x^{obs}_j)\right] $$

where our pointwise log liklihoods are

$$\log L(\theta,\sigma|x^{obs}_{ij}) = - \frac{1}{2} \log 2\pi -  \log\sigma_i +\frac{(x^{obs}_{ij} - f_i(t_j, \theta))^2}{2\sigma_i^2}$$

class GaussianLogLikelihood(pints.ProblemLogLikelihood):
    r"""
    Calculates a log-likelihood assuming independent Gaussian noise at each
    time point, and adds a parameter representing the standard deviation
    (sigma) of the noise on each output.
    For a noise level of ``sigma``, the likelihood becomes:
    .. math::
        L(\theta, \sigma|\boldsymbol{x})
            = p(\boldsymbol{x} | \theta, \sigma)
            = \prod_{j=1}^{n_t} \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(
                -\frac{(x_j - f_j(\theta))^2}{2\sigma^2}\right)
    leading to a log likelihood of:
    .. math::
        \log{L(\theta, \sigma|\boldsymbol{x})} =
            -\frac{n_t}{2} \log{2\pi}
            -n_t \log{\sigma}
            -\frac{1}{2\sigma^2}\sum_{j=1}^{n_t}{(x_j - f_j(\theta))^2}
    where ``n_t`` is the number of time points in the series, ``x_j`` is the
    sampled data at time ``j`` and ``f_j`` is the simulated data at time ``j``.
    For a system with ``n_o`` outputs, this becomes
    .. math::
        \log{L(\theta, \sigma|\boldsymbol{x})} =
            -\frac{n_t n_o}{2}\log{2\pi}
            -\sum_{i=1}^{n_o}{ {n_t}\log{\sigma_i} }
            -\sum_{i=1}^{n_o}{\left[
                \frac{1}{2\sigma_i^2}\sum_{j=1}^{n_t}{(x_j - f_j(\theta))^2}
             \right]}
    Extends :class:`ProblemLogLikelihood`.
    Parameters
    ----------
    problem
        A :class:`SingleOutputProblem` or :class:`MultiOutputProblem`. For a
        single-output problem a single parameter is added, for a multi-output
        problem ``n_outputs`` parameters are added.
    """

    def __init__(self, problem):
        super(GaussianLogLikelihood, self).__init__(problem)

        # Get number of times, number of outputs
        self._nt = len(self._times)
        self._no = problem.n_outputs()

        # Add parameters to problem
        self._n_parameters = problem.n_parameters() + self._no

        # Pre-calculate parts
        self._logn = 0.5 * np.log(2 * np.pi)
        
        #set up pointwise loglikelihoods
        self._last_pointwise_loglikelihoods = None

    def __call__(self, x):
        pointwise = self.create_pointwise_loglikelihoods(x)
        self._last_pointwise_loglikelihoods = pointwise
        return np.sum(pointwise)
    
    def create_pointwise_loglikelihoods(self, parameters):
        """
        Returns a matrix of size nt x no containing the loglikelihood of each observation and at each time point 
        with the given parameters
        """
        sigma = np.asarray(parameters[-self._no:])
        error = self._values - self._problem.evaluate(parameters[:-self._no])
        return -0.5 * np.log(2 * np.pi) - np.log(sigma) - error**2/ (2 * sigma**2)
    
    def get_last_pointwise_loglikelihoods(self):
        return self._last_pointwise_loglikelihoods
        
    def evaluateS1(self, x):
        """ See :meth:`LogPDF.evaluateS1()`. """
        sigma = np.asarray(x[-self._no:])

        # Evaluate, and get residuals
        y, dy = self._problem.evaluateS1(x[:-self._no])

        # Reshape dy, in case we're working with a single-output problem
        dy = dy.reshape(self._nt, self._no, self._n_parameters - self._no)

        # Note: Must be (data - simulation), sign now matters!
        r = self._values - y

        # Calculate log-likelihood
        L = self.__call__(x)

        # Calculate derivatives in the model parameters
        dL = np.sum(
            (sigma**(-2.0) * np.sum((r.T * dy.T).T, axis=0).T).T, axis=0)

        # Calculate derivative wrt sigma
        dsigma = -self._nt / sigma + sigma**(-3.0) * np.sum(r**2, axis=0)
        dL = np.concatenate((dL, np.array(list(dsigma))))

        # Return
        return L, dL

### Multiplicative Gaussian likelihood

If we assume that the distrbution of observations around the true value is relative to the magnitude of the true value, i.e. 
$$
X^{obs} = f\left(t, \theta\right) + \sigma f\left(t, \theta\right)^\eta \epsilon,
$$
where $ \epsilon \sim N\left(0,1\right)$.

Then the likelihood of our data is given by

$$ \log L\left(\theta, \sigma, \eta | X^{obs}\right) = \sum_{i=1}^{n_t}\sum_{j=1}^{n_t}\left[\log L\left(\theta,\sigma, \eta|x^{obs}_{ij}\right)\right] $$

where our pointwise log liklihoods are

$$\log L\left(\theta,\sigma, \eta|x^{obs}_{ij}\right) = - \frac{1}{2} \log 2\pi - \log f_i\left(t_j, \theta\right)^{\eta_i}\sigma_i -\frac{\left(x^{obs}_{ij} - f_i\left(t_j, \theta\right)\right)^2}{2\left(f_i\left(t_j, \theta\right)^{\eta_i}\sigma_i\right)^2}$$

class MultiplicativeGaussianLogLikelihood(pints.ProblemLogLikelihood):
    r"""
    Calculates the log-likelihood for a time-series model assuming a
    heteroscedastic Gaussian error of the model predictions
    :math:`f(t, \theta )`.
    This likelihood introduces two new scalar parameters for each dimension of
    the model output: an exponential power :math:`\eta` and a scale
    :math:`\sigma`.
    A heteroscedascic Gaussian noise model assumes that the observable
    :math:`X` is Gaussian distributed around the model predictions
    :math:`f(t, \theta )` with a standard deviation that scales with
    :math:`f(t, \theta )`
    .. math::
        X(t) = f(t, \theta) + \sigma f(t, \theta)^\eta v(t)
    where :math:`v(t)` is a standard i.i.d. Gaussian random variable
    .. math::
        v(t) \sim \mathcal{N}(0, 1).
    This model leads to a log likelihood of the model parameters of
    .. math::
        \log{L(\theta, \eta , \sigma | X^{\text{obs}})} =
            -\frac{n_t}{2} \log{2 \pi}
            -\sum_{i=1}^{n_t}{\log{f(t_i, \theta)^\eta \sigma}}
            -\frac{1}{2}\sum_{i=1}^{n_t}\left(
                \frac{X^{\text{obs}}_{i} - f(t_i, \theta)}
                {f(t_i, \theta)^\eta \sigma}\right) ^2,
    where :math:`n_t` is the number of time points in the series, and
    :math:`X^{\text{obs}}_{i}` the measurement at time :math:`t_i`.
    For a system with :math:`n_o` outputs, this becomes
    .. math::
        \log{L(\theta, \eta , \sigma | X^{\text{obs}})} =
            -\frac{n_t n_o}{2} \log{2 \pi}
            -\sum ^{n_o}_{j=1}\sum_{i=1}^{n_t}{\log{f_j(t_i, \theta)^\eta
            \sigma _j}}
            -\frac{1}{2}\sum ^{n_o}_{j=1}\sum_{i=1}^{n_t}\left(
                \frac{X^{\text{obs}}_{ij} - f_j(t_i, \theta)}
                {f_j(t_i, \theta)^\eta \sigma _j}\right) ^2,
    where :math:`n_o` is the number of outputs of the model, and
    :math:`X^{\text{obs}}_{ij}` the measurement of output :math:`j` at
    time point :math:`t_i`.
    Extends :class:`ProblemLogLikelihood`.
    Parameters
    ----------
    ``problem``
        A :class:`SingleOutputProblem` or :class:`MultiOutputProblem`. For a
        single-output problem two parameters are added (:math:`\eta`,
        :math:`\sigma`), for a multi-output problem 2 times :math:`n_o`
        parameters are added.
    ``fix_eta``
        If None eta will not be fixed and will be considered as a parameter. 
        If set to a number eta will be fixed to that number for all outputs.
        If set to a list of numbers eta_i will be set to the i-th number in
        the list
    """

    def __init__(self, problem, fix_eta=None):
        super(MultiplicativeGaussianLogLikelihood, self).__init__(problem)

        # Get number of times and number of outputs
        self._nt = len(self._times)
        no = problem.n_outputs()
        self._fix_eta = fix_eta
        if self._fix_eta == None:
            self._np = 2 * no  # 2 parameters added per output
        else:
            self._np = no  # 1 parameters added per output
            

        # Add parameters to problem
        self._n_parameters = problem.n_parameters() + self._np

        # Pre-calculate the constant part of the likelihood
        self._logn = 0.5 * np.log(2 * np.pi)
    
    def __call__(self, x):
        pointwise = self.create_pointwise_loglikelihoods(x)
        self._last_pointwise_loglikelihoods = pointwise
        return np.sum(pointwise)
    
    def create_pointwise_loglikelihoods(self, parameters):
        """
        Returns a matrix of size nt x no containing the loglikelihood of each observation and at each time point 
        with the given parameters
        """
        noise_parameters = np.asarray(parameters[-self._np:])
        if self._fix_eta == None:
            eta = np.asarray(noise_parameters[0::2])
            sigma = np.asarray(noise_parameters[1::2])
        else:
            sigma = noise_parameters
            eta = self._fix_eta
        
        
#         Evaluate function (n_times, n_output)
        function_values = self._problem.evaluate(parameters[:-self._np])
        error = self._values - function_values
        noise_term = function_values**eta * sigma
        return -0.5 * np.log(2 * np.pi) - np.log(noise_term) - error**2/ (2 * noise_term**2)
    
    def get_last_pointwise_loglikelihoods(self):
        return self._last_pointwise_loglikelihoods
    
#     def __call__(self, x):
#         # Get noise parameters
#         noise_parameters = x[-self._np:]
#         eta = np.asarray(noise_parameters[0::2])
#         sigma = np.asarray(noise_parameters[1::2])

#         # Evaluate function (n_times, n_output)
#         function_values = self._problem.evaluate(x[:-self._np])

#         # Compute likelihood
#         log_likelihood = \
#             -self._logn - np.sum(
#                 np.sum(np.log(function_values**eta * sigma), axis=0)
#                 + 0.5 / sigma**2 * np.sum(
#                     (self._values - function_values)**2
#                     / function_values ** (2 * eta), axis=0))

#         return log_likelihood

class MultiplicativeGaussianLogLikelihoodFixEta(pints.ProblemLogLikelihood):
    r"""
    Single noise terms
    
    Calculates the log-likelihood for a time-series model assuming a single
    heteroscedastic Gaussian error of the model predictions
    :math:`f(t, \theta )`.
    This likelihood introduces two new scalar parameters, both of which are constant across all dimensions of
    the model output: an exponential power :math:`\eta` and a scale
    :math:`\sigma`.
    A heteroscedascic Gaussian noise model assumes that the observable
    :math:`X` is Gaussian distributed around the model predictions
    :math:`f(t, \theta )` with a standard deviation that scales with
    :math:`f(t, \theta )`
    .. math::
        X(t) = f(t, \theta) + \sigma f(t, \theta)^\eta v(t)
    where :math:`v(t)` is a standard i.i.d. Gaussian random variable
    .. math::
        v(t) \sim \mathcal{N}(0, 1).
    This model leads to a log likelihood of the model parameters of
    .. math::
        \log{L(\theta, \eta , \sigma | X^{\text{obs}})} =
            -\frac{n_t}{2} \log{2 \pi}
            -\sum_{i=1}^{n_t}{\log{f(t_i, \theta)^\eta \sigma}}
            -\frac{1}{2}\sum_{i=1}^{n_t}\left(
                \frac{X^{\text{obs}}_{i} - f(t_i, \theta)}
                {f(t_i, \theta)^\eta \sigma}\right) ^2,
    where :math:`n_t` is the number of time points in the series, and
    :math:`X^{\text{obs}}_{i}` the measurement at time :math:`t_i`.
    For a system with :math:`n_o` outputs, this becomes
    .. math::
        \log{L(\theta, \eta , \sigma | X^{\text{obs}})} =
            -\frac{n_t n_o}{2} \log{2 \pi}
            -\sum ^{n_o}_{j=1}\sum_{i=1}^{n_t}{\log{f_j(t_i, \theta)^\eta
            \sigma}}
            -\frac{1}{2}\sum ^{n_o}_{j=1}\sum_{i=1}^{n_t}\left(
                \frac{X^{\text{obs}}_{ij} - f_j(t_i, \theta)}
                {f_j(t_i, \theta)^\eta \sigma}\right) ^2,
    where :math:`n_o` is the number of outputs of the model, and
    :math:`X^{\text{obs}}_{ij}` the measurement of output :math:`j` at
    time point :math:`t_i`.
    Extends :class:`ProblemLogLikelihood`.
    Parameters
    ----------
    ``problem``
        A :class:`SingleOutputProblem` or :class:`MultiOutputProblem`. For both
        single-output problems and multi-output problems, two parameters are added (:math:`\eta`,
        :math:`\sigma`).
    """

    def __init__(self, problem):
        super(MultiplicativeGaussianLogLikelihoodFixEta, self).__init__(problem)

        # Get number of times and number of outputs
        self._nt = len(self._times)
        no = problem.n_outputs()
        self._np = 1  # 1 parameters added

        # Add parameters to problem
        self._n_parameters = problem.n_parameters() + self._np

        # Pre-calculate the constant part of the likelihood
        self._logn = 0.5 * self._nt * no * np.log(2 * np.pi)

    def __call__(self, x):
        # Get noise parameters
        noise_parameters = x[-self._np:]
        eta = 1 # np.asarray(noise_parameters[0])
        sigma = np.asarray(noise_parameters[0])

        # Evaluate function (n_times, n_output)
        function_values = self._problem.evaluate(x[:-self._np])

        # Compute likelihood
        log_likelihood = \
            -self._logn - np.sum(
                np.sum(np.log(function_values**eta * sigma), axis=0)
                + 0.5 / sigma**2 * np.sum(
                    (self._values - function_values)**2
                    / function_values ** (2 * eta), axis=0))

        return log_likelihood
    
    def pointwiseLogliklihoods(self, parameters):
        # Get parameters from input
        noise_parameters = np.asarray(parameters[-self._np:])
        sigma_base = noise_parameters[:self._no]
        eta = noise_parameters[self._no:2 * self._no]
        sigma_rel = noise_parameters[2 * self._no:]

        # Evaluate noise-free model (n_times, n_outputs)
        function_values = self._problem.evaluate(parameters[:-self._np])

        # Compute error (n_times, n_outputs)
        error = self._values - function_values

        # Compute total standard deviation
        sigma_tot = sigma_base + sigma_rel * function_values**eta

        # Compute log-likelihood
        # (inner sums over time points, outer sum over parameters)
        log_likelihood = self._logn - np.sum(
            np.sum(np.log(sigma_tot), axis=0)
            + 0.5 * np.sum(error**2 / sigma_tot**2, axis=0))

        return log_likelihood
        

### Constant and Multiplicative Gaussian likelihood

If we assume that the distrbution of observations around the true value has both a constant and multiplicative part, i.e. 
$$
X^{obs} = f\left(t, \theta\right) + \left(\sigma_{base} + \sigma_{rel} f\left(t, \theta\right)^\eta\right) \epsilon,
$$
where $ \epsilon \sim N\left(0,1\right)$.

Then the likelihood of our data is given by

$$ \log L\left(\theta,\sigma_{base}, \sigma_{rel}, \eta | X^{obs}\right) = \sum_{i=1}^{n_t}\sum_{j=1}^{n_t}\left[\log L\left(\theta,\sigma_{base}, \sigma_{rel}, \eta|x^{obs}_{ij}\right)\right] $$

where our pointwise log liklihoods are

$$\log L\left(\theta,\sigma, \eta|x^{obs}_{ij}\right) = - \frac{1}{2} \log 2\pi - \log \sigma_{tot,ij} -\frac{\left(x^{obs}_{ij} - f_i\left(t_j, \theta\right)\right)^2}{2\sigma_{tot,ij}^2}$$

and $\sigma_{tot,ij} = \sigma_{base, i} + f_i\left(t_j, \theta\right)^{\eta_i}\sigma_{rel, i}$

class ConstantAndMultiplicativeGaussianLogLikelihood(
        pints.ProblemLogLikelihood):
    r"""
    Calculates the log-likelihood assuming a mixed error model of a
    Gaussian base-level noise and a Gaussian heteroscedastic noise.
    For a time series model :math:`f(t| \theta)` with parameters :math:`\theta`
    , the ConstantAndMultiplicativeGaussianLogLikelihood assumes that the
    model predictions :math:`X` are Gaussian distributed according to
    .. math::
        X(t| \theta , \sigma _{\text{base}}, \sigma _{\text{rel}}) =
        f(t| \theta) + (\sigma _{\text{base}} + \sigma _{\text{rel}}
        f(t| \theta)^\eta ) \, \epsilon ,
    where :math:`\epsilon` is a i.i.d. standard Gaussian random variable
    .. math::
        \epsilon \sim \mathcal{N}(0, 1).
    For each output in the problem, this likelihood introduces three new scalar
    parameters: a base-level scale :math:`\sigma _{\text{base}}`; an
    exponential power :math:`\eta`; and a scale relative to the model output
    :math:`\sigma _{\text{rel}}`.
    The resulting log-likelihood of a constant and multiplicative Gaussian
    error model is
    .. math::
        \log L(\theta, \sigma _{\text{base}}, \eta ,
        \sigma _{\text{rel}} | X^{\text{obs}})
        = -\frac{n_t}{2} \log 2 \pi
        -\sum_{i=1}^{n_t}\log \sigma _{\text{tot}, i}
        - \sum_{i=1}^{n_t}
        \frac{(X^{\text{obs}}_i - f(t_i| \theta))^2}
        {2\sigma ^2_{\text{tot}, i}},
    where :math:`n_t` is the number of measured time points in the time series,
    :math:`X^{\text{obs}}_i` is the observation at time point :math:`t_i`, and
    :math:`\sigma _{\text{tot}, i}=\sigma _{\text{base}} +\sigma _{\text{rel}}
    f(t_i| \theta)^\eta` is the total standard deviation of the error at time
    :math:`t_i`.
    For a system with :math:`n_o` outputs, this becomes
    .. math::
        \log L(\theta, \sigma _{\text{base}}, \eta ,
        \sigma _{\text{rel}} | X^{\text{obs}})
        = -\frac{n_tn_o}{2} \log 2 \pi
        -\sum_{j=1}^{n_0}\sum_{i=1}^{n_t}\log \sigma _{\text{tot}, ij}
        - \sum_{j=1}^{n_0}\sum_{i=1}^{n_t}
        \frac{(X^{\text{obs}}_{ij} - f_j(t_i| \theta))^2}
        {2\sigma ^2_{\text{tot}, ij}},
    where :math:`n_o` is the number of outputs of the model,
    :math:`X^{\text{obs}}_{ij}` is the observation at time point :math:`t_i`
    of output :math:`j`, and
    :math:`\sigma _{\text{tot}, ij}=\sigma _{\text{base}, j} +
    \sigma _{\text{rel}, j}f_j(t_i| \theta)^{\eta _j}` is the total standard
    deviation of the error at time :math:`t_i` of output :math:`j`.
    Extends :class:`ProblemLogLikelihood`.
    Parameters
    ----------
    ``problem``
        A :class:`SingleOutputProblem` or :class:`MultiOutputProblem`. For a
        single-output problem three parameters are added
        (:math:`\sigma _{\text{base}}`, :math:`\eta`,
        :math:`\sigma _{\text{rel}}`),
        for a multi-output problem :math:`3n_o` parameters are added
        (:math:`\sigma _{\text{base},1},\ldots , \sigma _{\text{base},n_o},
        \eta _1,\ldots , \eta _{n_o}, \sigma _{\text{rel},1}, \ldots ,
        \sigma _{\text{rel},n_o})`.
    """

    def __init__(self, problem, fix_noise=None):
        super(ConstantAndMultiplicativeGaussianLogLikelihood, self).__init__(
            problem)

        # Get number of times and number of noise parameters
        self._nt = len(self._times)
        self._no = problem.n_outputs()
        
        if fix_param==None:
            self._fix_param = [None]*3
        else:
            self._fix_param = fix_param
        self._np = (self._fix_param.count(None)) * self._no
        
        # Add parameters to problem
        self._n_parameters = problem.n_parameters() + self._np

        # Pre-calculate the constant part of the likelihood
        self._logn = -0.5 * np.log(2 * np.pi)

    def __call__(self, x):
        pointwise = self.create_pointwise_loglikelihoods(x)
        self._last_pointwise_loglikelihoods = pointwise
        return np.sum(pointwise)
    
    def create_pointwise_loglikelihoods(self, parameters):
        """
        Returns a matrix of size nt x no containing the loglikelihood of each observation and at each time point 
        with the given parameters
        """
        noise_parameters = np.asarray(parameters[-self._np:])
        if self._fix_param[0] == None:
            sigma_base = noise_parameters[:self._no]
        else:
            sigma_base = self._fix_param[0]
        if self._fix_param[1] == None:
            eta = noise_parameters[self._no:2 * self._no]
        else:
            eta = self._fix_param[1]
        if self._fix_param[2] == None:
            sigma_rel = noise_parameters[2 * self._no:]
        else:
            sigma_rel = self._fix_param[2]
        
#       Evaluate function (n_times, n_output)
        function_values = self._problem.evaluate(parameters[:-self._np])
        error = self._values - function_values
        sigma_tot = sigma_base + sigma_rel * function_values**eta
        return -0.5 * np.log(2 * np.pi) - np.log(sigma_tot) - 0.5* error**2/ (sigma_tot**2)
    
    def get_last_pointwise_loglikelihoods(self):
        return self._last_pointwise_loglikelihoods
    
#     def __call__(self, parameters):
#         # Get parameters from input
#         noise_parameters = np.asarray(parameters[-self._np:])
#         sigma_base = noise_parameters[:self._no]
#         eta = noise_parameters[self._no:2 * self._no]
#         sigma_rel = noise_parameters[2 * self._no:]

#         # Evaluate noise-free model (n_times, n_outputs)
#         function_values = self._problem.evaluate(parameters[:-self._np])

#         # Compute error (n_times, n_outputs)
#         error = self._values - function_values

#         # Compute total standard deviation
#         sigma_tot = sigma_base + sigma_rel * function_values**eta

#         # Compute log-likelihood
#         # (inner sums over time points, outer sum over parameters)
#         log_likelihood = self._logn - np.sum(
#             np.sum(np.log(sigma_tot), axis=0)
#             + 0.5 * np.sum(error**2 / sigma_tot**2, axis=0))

#         return log_likelihood

    def evaluateS1(self, parameters):
        r"""
        See :meth:`LogPDF.evaluateS1()`.
        The partial derivatives of the log-likelihood w.r.t. the model
        parameters are
        .. math::
            \frac{\partial \log L}{\partial \theta _k}
            =& -\sum_{i,j}\sigma _{\text{rel},j}\eta _j\frac{
            f_j(t_i| \theta)^{\eta _j-1}}
            {\sigma _{\text{tot}, ij}}
            \frac{\partial f_j(t_i| \theta)}{\partial \theta _k}
            + \sum_{i,j}
            \frac{X^{\text{obs}}_{ij} - f_j(t_i| \theta)}
            {\sigma ^2_{\text{tot}, ij}}
            \frac{\partial f_j(t_i| \theta)}{\partial \theta _k} \\
            &+\sum_{i,j}\sigma _{\text{rel},j}\eta _j
            \frac{(X^{\text{obs}}_{ij} - f_j(t_i| \theta))^2}
            {\sigma ^3_{\text{tot}, ij}}f_j(t_i| \theta)^{\eta _j-1}
            \frac{\partial f_j(t_i| \theta)}{\partial \theta _k} \\
            \frac{\partial \log L}{\partial \sigma _{\text{base}, j}}
            =& -\sum ^{n_t}_{i=1}\frac{1}{\sigma _{\text{tot}, ij}}
            +\sum ^{n_t}_{i=1}
            \frac{(X^{\text{obs}}_{ij} - f_j(t_i| \theta))^2}
            {\sigma ^3_{\text{tot}, ij}} \\
            \frac{\partial \log L}{\partial \eta _j}
            =& -\sigma _{\text{rel},j}\eta _j\sum ^{n_t}_{i=1}
            \frac{f_j(t_i| \theta)^{\eta _j}\log f_j(t_i| \theta)}
            {\sigma _{\text{tot}, ij}}
            + \sigma _{\text{rel},j}\eta _j \sum ^{n_t}_{i=1}
            \frac{(X^{\text{obs}}_{ij} - f_j(t_i| \theta))^2}
            {\sigma ^3_{\text{tot}, ij}}f_j(t_i| \theta)^{\eta _j}
            \log f_j(t_i| \theta) \\
            \frac{\partial \log L}{\partial \sigma _{\text{rel},j}}
            =& -\sum ^{n_t}_{i=1}
            \frac{f_j(t_i| \theta)^{\eta _j}}{\sigma _{\text{tot}, ij}}
            + \sum ^{n_t}_{i=1}
            \frac{(X^{\text{obs}}_{ij} - f_j(t_i| \theta))^2}
            {\sigma ^3_{\text{tot}, ij}}f_j(t_i| \theta)^{\eta _j},
        where :math:`i` sums over the measurement time points and :math:`j`
        over the outputs of the model.
        """
        # Get parameters from input
        # Shape sigma_base, eta, sigma_rel = (n_outputs,)
        noise_parameters = np.asarray(parameters[-self._np:])
        sigma_base = noise_parameters[:self._no]
        eta = noise_parameters[self._no:2 * self._no]
        sigma_rel = noise_parameters[-self._no:]

        # Evaluate noise-free model, and get residuals
        # y shape = (n_times,) or (n_times, n_outputs)
        # dy shape = (n_times, n_model_parameters) or
        # (n_times, n_outputs, n_model_parameters)
        y, dy = self._problem.evaluateS1(parameters[:-self._np])

        # Reshape y and dy, in case we're working with a single-output problem
        # Shape y = (n_times, n_outputs)
        # Shape dy = (n_model_parameters, n_times, n_outputs)
        y = y.reshape(self._nt, self._no)
        dy = np.transpose(
            dy.reshape(self._nt, self._no, self._n_parameters - self._np),
            axes=(2, 0, 1))

        # Compute error
        # Note: Must be (data - simulation), sign now matters!
        # Shape: (n_times, output)
        error = self._values.reshape(self._nt, self._no) - y

        # Compute total standard deviation
        sigma_tot = sigma_base + sigma_rel * y**eta

        # Compute likelihood
        L = self.__call__(parameters)

        # Compute derivative w.r.t. model parameters
        dtheta = -np.sum(sigma_rel * eta * np.sum(
            y**(eta - 1) * dy / sigma_tot, axis=1), axis=1) + \
            np.sum(error * dy / sigma_tot**2, axis=(1, 2)) + np.sum(
                sigma_rel * eta * np.sum(
                    error**2 * y**(eta - 1) * dy / sigma_tot**3, axis=1),
                axis=1)

        # Compute derivative w.r.t. sigma base
        dsigma_base = - np.sum(1 / sigma_tot, axis=0) + np.sum(
            error**2 / sigma_tot**3, axis=0)

        # Compute derivative w.r.t. eta
        deta = -sigma_rel * (
            np.sum(y**eta * np.log(y) / sigma_tot, axis=0) -
            np.sum(
                error**2 / sigma_tot**3 * y**eta * np.log(y),
                axis=0))

        # Compute derivative w.r.t. sigma rel
        dsigma_rel = -np.sum(y**eta / sigma_tot, axis=0) + np.sum(
            error**2 / sigma_tot**3 * y**eta, axis=0)

        # Collect partial derivatives
        dL = np.hstack((dtheta, dsigma_base, deta, dsigma_rel))

        # Return
        return L, dL

class ConstantAndMultiplicativeGaussianLogLikelihoodFixEta(
        pints.ProblemLogLikelihood):
    r"""
    Calculates the log-likelihood assuming a mixed error model of a
    Gaussian base-level noise and a Gaussian heteroscedastic noise.
    For a time series model :math:`f(t| \theta)` with parameters :math:`\theta`
    , the ConstantAndMultiplicativeGaussianLogLikelihood assumes that the
    model predictions :math:`X` are Gaussian distributed according to
    .. math::
        X(t| \theta , \sigma _{\text{base}}, \sigma _{\text{rel}}) =
        f(t| \theta) + (\sigma _{\text{base}} + \sigma _{\text{rel}}
        f(t| \theta)^\eta ) \, \epsilon ,
    where :math:`\epsilon` is a i.i.d. standard Gaussian random variable
    .. math::
        \epsilon \sim \mathcal{N}(0, 1).
    For each output in the problem, this likelihood introduces three new scalar
    parameters: a base-level scale :math:`\sigma _{\text{base}}`; an
    exponential power :math:`\eta`; and a scale relative to the model output
    :math:`\sigma _{\text{rel}}`.
    The resulting log-likelihood of a constant and multiplicative Gaussian
    error model is
    .. math::
        \log L(\theta, \sigma _{\text{base}}, \eta ,
        \sigma _{\text{rel}} | X^{\text{obs}})
        = -\frac{n_t}{2} \log 2 \pi
        -\sum_{i=1}^{n_t}\log \sigma _{\text{tot}, i}
        - \sum_{i=1}^{n_t}
        \frac{(X^{\text{obs}}_i - f(t_i| \theta))^2}
        {2\sigma ^2_{\text{tot}, i}},
    where :math:`n_t` is the number of measured time points in the time series,
    :math:`X^{\text{obs}}_i` is the observation at time point :math:`t_i`, and
    :math:`\sigma _{\text{tot}, i}=\sigma _{\text{base}} +\sigma _{\text{rel}}
    f(t_i| \theta)^\eta` is the total standard deviation of the error at time
    :math:`t_i`.
    For a system with :math:`n_o` outputs, this becomes
    .. math::
        \log L(\theta, \sigma _{\text{base}}, \eta ,
        \sigma _{\text{rel}} | X^{\text{obs}})
        = -\frac{n_tn_o}{2} \log 2 \pi
        -\sum_{j=1}^{n_0}\sum_{i=1}^{n_t}\log \sigma _{\text{tot}, ij}
        - \sum_{j=1}^{n_0}\sum_{i=1}^{n_t}
        \frac{(X^{\text{obs}}_{ij} - f_j(t_i| \theta))^2}
        {2\sigma ^2_{\text{tot}, ij}},
    where :math:`n_o` is the number of outputs of the model,
    :math:`X^{\text{obs}}_{ij}` is the observation at time point :math:`t_i`
    of output :math:`j`, and
    :math:`\sigma _{\text{tot}, ij}=\sigma _{\text{base}, j} +
    \sigma _{\text{rel}, j}f_j(t_i| \theta)^{\eta _j}` is the total standard
    deviation of the error at time :math:`t_i` of output :math:`j`.
    Extends :class:`ProblemLogLikelihood`.
    Parameters
    ----------
    ``problem``
        A :class:`SingleOutputProblem` or :class:`MultiOutputProblem`. For a
        single-output problem three parameters are added
        (:math:`\sigma _{\text{base}}`, :math:`\eta`,
        :math:`\sigma _{\text{rel}}`),
        for a multi-output problem :math:`3n_o` parameters are added
        (:math:`\sigma _{\text{base},1},\ldots , \sigma _{\text{base},n_o},
        \eta _1,\ldots , \eta _{n_o}, \sigma _{\text{rel},1}, \ldots ,
        \sigma _{\text{rel},n_o})`.
    """

    def __init__(self, problem):
        super(ConstantAndMultiplicativeGaussianLogLikelihoodFixEta, self).__init__(
            problem)

        # Get number of times and number of noise parameters
        self._nt = len(self._times)
        self._no = problem.n_outputs()
        self._np = 2 * self._no

        # Add parameters to problem
        self._n_parameters = problem.n_parameters() + self._np

        # Pre-calculate the constant part of the likelihood
        self._logn = -0.5 * self._nt * self._no * np.log(2 * np.pi)

    def __call__(self, parameters):
        # Get parameters from input
        noise_parameters = np.asarray(parameters[-self._np:])
        sigma_base = noise_parameters[:self._no]
        eta = 1
        sigma_rel = noise_parameters[self._no:]

        # Evaluate noise-free model (n_times, n_outputs)
        function_values = self._problem.evaluate(parameters[:-self._np])

        # Compute error (n_times, n_outputs)
        error = self._values - function_values

        # Compute total standard deviation
        sigma_tot = sigma_base + sigma_rel * function_values**eta

        # Compute log-likelihood
        # (inner sums over time points, outer sum over parameters)
        log_likelihood = self._logn - np.sum(
            np.sum(np.log(sigma_tot), axis=0)
            + 0.5 * np.sum(error**2 / sigma_tot**2, axis=0))

        return log_likelihood

    def evaluateS1(self, parameters):
        r"""
        See :meth:`LogPDF.evaluateS1()`.
        The partial derivatives of the log-likelihood w.r.t. the model
        parameters are
        .. math::
            \frac{\partial \log L}{\partial \theta _k}
            =& -\sum_{i,j}\sigma _{\text{rel},j}\eta _j\frac{
            f_j(t_i| \theta)^{\eta _j-1}}
            {\sigma _{\text{tot}, ij}}
            \frac{\partial f_j(t_i| \theta)}{\partial \theta _k}
            + \sum_{i,j}
            \frac{X^{\text{obs}}_{ij} - f_j(t_i| \theta)}
            {\sigma ^2_{\text{tot}, ij}}
            \frac{\partial f_j(t_i| \theta)}{\partial \theta _k} \\
            &+\sum_{i,j}\sigma _{\text{rel},j}\eta _j
            \frac{(X^{\text{obs}}_{ij} - f_j(t_i| \theta))^2}
            {\sigma ^3_{\text{tot}, ij}}f_j(t_i| \theta)^{\eta _j-1}
            \frac{\partial f_j(t_i| \theta)}{\partial \theta _k} \\
            \frac{\partial \log L}{\partial \sigma _{\text{base}, j}}
            =& -\sum ^{n_t}_{i=1}\frac{1}{\sigma _{\text{tot}, ij}}
            +\sum ^{n_t}_{i=1}
            \frac{(X^{\text{obs}}_{ij} - f_j(t_i| \theta))^2}
            {\sigma ^3_{\text{tot}, ij}} \\
            \frac{\partial \log L}{\partial \eta _j}
            =& -\sigma _{\text{rel},j}\eta _j\sum ^{n_t}_{i=1}
            \frac{f_j(t_i| \theta)^{\eta _j}\log f_j(t_i| \theta)}
            {\sigma _{\text{tot}, ij}}
            + \sigma _{\text{rel},j}\eta _j \sum ^{n_t}_{i=1}
            \frac{(X^{\text{obs}}_{ij} - f_j(t_i| \theta))^2}
            {\sigma ^3_{\text{tot}, ij}}f_j(t_i| \theta)^{\eta _j}
            \log f_j(t_i| \theta) \\
            \frac{\partial \log L}{\partial \sigma _{\text{rel},j}}
            =& -\sum ^{n_t}_{i=1}
            \frac{f_j(t_i| \theta)^{\eta _j}}{\sigma _{\text{tot}, ij}}
            + \sum ^{n_t}_{i=1}
            \frac{(X^{\text{obs}}_{ij} - f_j(t_i| \theta))^2}
            {\sigma ^3_{\text{tot}, ij}}f_j(t_i| \theta)^{\eta _j},
        where :math:`i` sums over the measurement time points and :math:`j`
        over the outputs of the model.
        """
        # Get parameters from input
        # Shape sigma_base, eta, sigma_rel = (n_outputs,)
        noise_parameters = np.asarray(parameters[-self._np:])
        sigma_base = noise_parameters[:self._no]
        eta = 1
        sigma_rel = noise_parameters[self._no:]

        # Evaluate noise-free model, and get residuals
        # y shape = (n_times,) or (n_times, n_outputs)
        # dy shape = (n_times, n_model_parameters) or
        # (n_times, n_outputs, n_model_parameters)
        y, dy = self._problem.evaluateS1(parameters[:-self._np])

        # Reshape y and dy, in case we're working with a single-output problem
        # Shape y = (n_times, n_outputs)
        # Shape dy = (n_model_parameters, n_times, n_outputs)
        y = y.reshape(self._nt, self._no)
        dy = np.transpose(
            dy.reshape(self._nt, self._no, self._n_parameters - self._np),
            axes=(2, 0, 1))

        # Compute error
        # Note: Must be (data - simulation), sign now matters!
        # Shape: (n_times, output)
        error = self._values.reshape(self._nt, self._no) - y

        # Compute total standard deviation
        sigma_tot = sigma_base + sigma_rel * y**eta

        # Compute likelihood
        L = self.__call__(parameters)

        # Compute derivative w.r.t. model parameters
        dtheta = -np.sum(sigma_rel * eta * np.sum(
            y**(eta - 1) * dy / sigma_tot, axis=1), axis=1) + \
            np.sum(error * dy / sigma_tot**2, axis=(1, 2)) + np.sum(
                sigma_rel * eta * np.sum(
                    error**2 * y**(eta - 1) * dy / sigma_tot**3, axis=1),
                axis=1)

        # Compute derivative w.r.t. sigma base
        dsigma_base = - np.sum(1 / sigma_tot, axis=0) + np.sum(
            error**2 / sigma_tot**3, axis=0)

        # Compute derivative w.r.t. eta
        deta = -sigma_rel * (
            np.sum(y**eta * np.log(y) / sigma_tot, axis=0) -
            np.sum(
                error**2 / sigma_tot**3 * y**eta * np.log(y),
                axis=0))

        # Compute derivative w.r.t. sigma rel
        dsigma_rel = -np.sum(y**eta / sigma_tot, axis=0) + np.sum(
            error**2 / sigma_tot**3 * y**eta, axis=0)

        # Collect partial derivatives
        dL = np.hstack((dtheta, dsigma_base, deta, dsigma_rel))

        # Return
        return L, dL

### Make html

from traitlets.config import Config
import nbformat as nbf
from nbconvert.exporters import HTMLExporter
from nbconvert.preprocessors import TagRemovePreprocessor

# Setup config
c = Config()

# Configure tag removal - be sure to tag your cells to remove  using the
# words remove_cell to remove cells. You can also modify the code to use
# a different tag word
c.TagRemovePreprocessor.remove_cell_tags = ('remove_cell',)
c.TagRemovePreprocessor.remove_all_outputs_tags = ('remove_output',)
c.TagRemovePreprocessor.remove_input_tags = ('remove_input',)
c.TagRemovePreprocessor.enabled = True

# Configure and run out exporter
c.HTMLExporter.preprocessors = ["nbconvert.preprocessors.TagRemovePreprocessor"]

exporter = HTMLExporter(config=c)
exporter.register_preprocessor(TagRemovePreprocessor(config=c),True)

# Configure and run our exporter - returns a tuple - first element with html,
# second with notebook metadata
output = HTMLExporter(config=c).from_filename("PD_naive_pooled_inference.ipynb")

# Write to output html file
with open("PD_naive_pooled_inference.html",  "w") as f:
    f.write(output[0])


